{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "Q3M7woqMfzB-"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhrits/LLM-Engineering-Foundations-to-SLMs/blob/main/05_Next-Token-Prediction/Decoding_From_Logits_to_the_Speculative_Decoding_and_Guard_Rails_Hardmode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoding: From Logits to the Speculative Decoding and Guard Rails\n",
        "\n",
        "We've covered everything from loss to logits - and now we're going to dive in to how we select the next token in details, and all the options we can use to do it!\n",
        "\n",
        "Breakout Room #1: Logits to Tokens\n",
        "- Task 1: Dependencies\n",
        "- Task 2: Generating Tokens!\n",
        "  - ğŸ—ï¸ Activity #1:\n",
        "- Task 3: Data Preprocessing\n",
        "  - â“Question #1\n",
        "- Task 4: Alternate Decoding Examples:\n",
        "  - ğŸ‘ªâ“ Discussion Question #1\n",
        "\n",
        "Breakout Room #2: Speculative Decoding and Guard Rails\n",
        "- Task 5: Speculative Decoding\n",
        "  - â“ Discussion Question #2\n",
        "- Task 6: Guard Rails\n",
        "  - ğŸ‘ªâ“ Discussion Question #2"
      ],
      "metadata": {
        "id": "oBRtW67UDRiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Dependencies\n",
        "\n",
        "Today we'll be using a classic minamalist implementation of a decoder-only transformer model called `nanoGPT`, built by the one-and-only Andrej Karpathy - found [here](https://github.com/karpathy/nanoGPT/tree/master)!\n",
        "\n",
        "It does require a few dependencies - though most are covered by the default Colab environment.\n",
        "\n",
        "> NOTE: You will need to make sure you're in a GPU enabled environment for effective use of this notebook."
      ],
      "metadata": {
        "id": "Lu7EnLapFJYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU datasets tiktoken wandb tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj3ShLeSEvH3",
        "outputId": "a6f95d48-531c-4da0-9740-d73043a13e89"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBdaBurtA8NP",
        "outputId": "7b6368aa-dde2-4d10-c327-24b5dba2c5d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 682, done.\u001b[K\n",
            "remote: Total 682 (delta 0), reused 0 (delta 0), pack-reused 682 (from 1)\u001b[K\n",
            "Receiving objects: 100% (682/682), 952.47 KiB | 41.41 MiB/s, done.\n",
            "Resolving deltas: 100% (385/385), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nanoGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xjcjW0zEhbM",
        "outputId": "ec3c38ee-af7b-451f-f71e-3cd18d0c311b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Generating Tokens!\n",
        "\n",
        "Let's just try to do some inference and see what happens before we dig in."
      ],
      "metadata": {
        "id": "6ZtIRopJGfRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSKkqMHdErid",
        "outputId": "72df4a92-2256-4f12-bede-6fa1a4bad87a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "2024-12-07 21:32:11.131461: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-07 21:32:11.148441: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 21:32:11.169369: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 21:32:11.175681: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 21:32:11.190683: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 21:32:12.345914: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "What is the answer to life, the universe, and everything?\n",
            "\n",
            "One possibility is that they have a universe-wide perspective that allows for no contradictions. And, if so, then why is there contradiction?\n",
            "\n",
            "Another possibility is that the universe is one big joke. They are not the only joke in the universe. But it is not an empty universe.\n",
            "\n",
            "Hence the problems with the universe's Big Bang theory, which tells us that the universe started out in a singularity with a zero initial mass. The Big Bang is essentially consistent with\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll notice that we pass *in* text - and we receive *back* text from our model."
      ],
      "metadata": {
        "id": "i9VQDtfcGych"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ğŸ—ï¸ Activity #1:\n",
        "\n",
        "*Where* (in the architecture) exactly does NanoGPT go \"from logits to predicted token\"? Provide a screenshot from [this visualization](https://bbycroft.net/llm)!\n",
        "\n",
        "Based on the visualization, this is happening in \"Logits Softmax\" layer where the logits are converted to a probability distribution using softmax and then sampled from using different strategies. Finally, the sampled index is converted back to text using the tokenizer.\n",
        "\n",
        "![visualization](https://github.com/dhrits/LLM-Engineering-Foundations-to-SLMs/blob/main/05_Next-Token-Prediction/LogitsSoftmax.png?raw=1)"
      ],
      "metadata": {
        "id": "0MhDBLg9sU5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: How Does the LLM Generate Tokens\n",
        "\n",
        "So, we pas in text - and get text back - but how do we actually generate each token?\n",
        "\n",
        "You might have heard the term \"auto-regressive\" or \"causal\" kicking around when reading about LLMs - and what those terms, in a simplified sense, mean is straightfoward enough:\n",
        "\n",
        "- They take an input, and generate a single token\n",
        "- They append that token to the input and repeat this process for as long as we want it to repeat (or use heuristics to determine when to stop, such as when we see a stop token)\n",
        "\n",
        "Let's take a look at the function that does this in the `nanoGPT` repository.\n",
        "\n"
      ],
      "metadata": {
        "id": "NKNzRbwuG0M2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "@torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n",
        "```"
      ],
      "metadata": {
        "id": "2Jpj1ytjH5--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is a Logit?!\n",
        "\n",
        "Technically - a logit is a \"raw unnormalized score\".\n",
        "\n",
        "However, we can think of them as scores for each token in our vocabulary. These scores aren't probabilities in and of themselves - but they can be easily converted to probabilities through the softmax function."
      ],
      "metadata": {
        "id": "tvDfy32YKnxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Temperature Doing?!\n",
        "\n",
        "While something like `top_k` makes intuitive sense - what in the heck is temperature doing here?\n",
        "\n",
        "In order to understand - let's look at a few examples!\n",
        "\n",
        "Starting with an easy `temperature = 1.0`.\n",
        "\n",
        "> NOTE: We'll also define our softmax function!"
      ],
      "metadata": {
        "id": "sRvnaByHLUr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    return(np.exp(x - np.max(x)) / np.exp(x - np.max(x)).sum())"
      ],
      "metadata": {
        "id": "lOWCrlUsMSnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "temperature = 1.0\n",
        "\n",
        "logits = np.array([6, 2, 7, 0.1, -8, 9])\n",
        "\n",
        "temp_scaled_logits = logits / temperature\n",
        "print(f\"Scaled Logits: {temp_scaled_logits}\")\n",
        "\n",
        "softmaxed_logits = softmax(temp_scaled_logits)\n",
        "print(f\"Softmax-ed Logits: {softmaxed_logits}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXpso0sYLdwI",
        "outputId": "9cde4f08-9503-40ea-b81f-c7f1008797a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled Logits: [ 6.   2.   7.   0.1 -8.   9. ]\n",
            "Softmax-ed Logits: [4.19729385e-02 7.68761185e-04 1.14094276e-01 1.14982549e-04\n",
            " 3.49017038e-08 8.43049007e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see - our logits are not changed, and our softmax output has quite a bit of variety - from `e-08` to `e-1`, meaning that our index with the score `9` is most likely to be selected, but it's not absurdly likely.\n",
        "\n",
        "Let's look at an example with a very low temperature next!"
      ],
      "metadata": {
        "id": "AbE3L_VKMByY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = 0.1\n",
        "\n",
        "logits = np.array([6, 2, 7, 0.1, -8, 9])\n",
        "\n",
        "temp_scaled_logits = logits / temperature\n",
        "print(f\"Scaled Logits: {temp_scaled_logits}\")\n",
        "\n",
        "softmaxed_logits = softmax(temp_scaled_logits)\n",
        "print(f\"Softmax-ed Logits: {softmaxed_logits}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE5fO2SSM3Ct",
        "outputId": "e7291a40-a919-479e-b7a0-3a52657762a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled Logits: [ 60.  20.  70.   1. -80.  90.]\n",
            "Softmax-ed Logits: [9.35762295e-14 3.97544973e-31 2.06115362e-09 2.22736356e-39\n",
            " 1.47889750e-74 9.99999998e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see - now that we changed our temperature to be very low - the index with score `9` is *vastly* more likely than any other option.\n",
        "\n",
        "This is the idea that a low (<1) temperature value will scale our logits to be larger - resulting in a sharper probability distribution after softmax.\n",
        "\n",
        "Let's look at a final example with a higher temperature."
      ],
      "metadata": {
        "id": "5VVVtg8_NQ06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = 100\n",
        "\n",
        "logits = np.array([6, 2, 7, 0.1, -8, 9])\n",
        "\n",
        "temp_scaled_logits = logits / temperature\n",
        "print(f\"Scaled Logits: {temp_scaled_logits}\")\n",
        "\n",
        "softmaxed_logits = softmax(temp_scaled_logits)\n",
        "print(f\"Softmax-ed Logits: {softmaxed_logits}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-tRO3UDNmt1",
        "outputId": "a89108a1-58d4-4227-e388-d5d971866060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled Logits: [ 0.06   0.02   0.07   0.001 -0.08   0.09 ]\n",
            "Softmax-ed Logits: [0.17201758 0.16527268 0.17374639 0.16216214 0.1495449  0.1772563 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100 --temperature=100.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYmie7meLtAG",
        "outputId": "cdf39af5-097c-47d4-d482-54ad6c37a0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "Overriding: temperature = 100.0\n",
            "2024-12-07 21:33:17.783616: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-07 21:33:17.800545: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 21:33:17.821367: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 21:33:17.827780: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 21:33:17.843020: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 21:33:19.012993: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "What is the answer to life, the universe, and everything? Have any prominent clerics yet responded they have? Religion columnist Keith Nearer considers â€” though keeps religion contained<|endoftext|>Police charge second Maryland inmate is now under search! If news breaking only had an 8 but kept u Peking for few tehtsts instead WN!!(19 mins 2):/police-indiewatson â€¦ wwwâ€¦â€¦#blackmagic420â€¦â€¦.(63 minute tape.. 1 long teute from da aikkkâ€¦[hobby][play with[SIB1 10\n",
            "c'\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can see that, while our index with score `9` is still the most likely - we can see that the probabilities are much closer together!"
      ],
      "metadata": {
        "id": "CBMM9M1VM7We"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### â“ Question #1:\n",
        "\n",
        "Why is the softmax operation so important for decoding?\n",
        "\n",
        "The softmax operation converts the raw unnormalized model outputs or logits int a probability distribution which can be sampled from. Once we have this distribution, we can apply various sampling strategies from statistics to output the next token."
      ],
      "metadata": {
        "id": "cZWakEUxtJlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Alternate Decoding Examples\n",
        "\n",
        "Let's look at a few other methods we could use to go from our logits to some text!\n",
        "\n",
        "We'll look at each component of the following script for each different method in detail as we go through them!"
      ],
      "metadata": {
        "id": "j6-dh6ebR250"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full Modified Generation Script"
      ],
      "metadata": {
        "id": "Q3M7woqMfzB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_sample_script = r'''\n",
        "\"\"\"\n",
        "Sample from a trained model with various decoding strategies, including repetition penalty\n",
        "\"\"\"\n",
        "import os\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tiktoken\n",
        "from model import GPTConfig, GPT\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
        "out_dir = 'out' # ignored if init_from is not 'resume'\n",
        "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "num_samples = 10 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "top_p = 0.0 # If set, use nucleus sampling\n",
        "beam_width = 0 # If set, use beam search with this width\n",
        "greedy = False # use greedy decoding\n",
        "repetition_penalty = 1.0 # 1.0 means no penalty, > 1.0 discourages repetition, < 1.0 encourages repetition\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "exec(open('configurator.py').read()) # overrides from command line or config file\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# model\n",
        "if init_from == 'resume':\n",
        "    # init from a model saved in a specific directory\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "elif init_from.startswith('gpt2'):\n",
        "    # init from a given GPT-2 model\n",
        "    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
        "\n",
        "# look for the meta pickle in case it is available in the dataset folder\n",
        "load_meta = False\n",
        "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
        "    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
        "    load_meta = os.path.exists(meta_path)\n",
        "if load_meta:\n",
        "    print(f\"Loading meta from {meta_path}...\")\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
        "    stoi, itos = meta['stoi'], meta['itos']\n",
        "    encode = lambda s: [stoi[c] for c in s]\n",
        "    decode = lambda l: ''.join([itos[i] for i in l])\n",
        "else:\n",
        "    # ok let's assume gpt-2 encodings by default\n",
        "    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "    decode = lambda l: enc.decode(l)\n",
        "\n",
        "# Top-P (nucleus) sampling\n",
        "def top_p_sampling(logits, p):\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "    sorted_indices_to_remove = cumulative_probs > p\n",
        "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "    sorted_indices_to_remove[..., 0] = 0\n",
        "    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "    probs = probs.masked_fill(indices_to_remove, 0.0)\n",
        "    return torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "# Beam Search with Repetition Penalty\n",
        "def beam_search(model, x, max_new_tokens, beam_width, repetition_penalty):\n",
        "    beams = [(x, 0)]\n",
        "    for _ in range(max_new_tokens):\n",
        "        candidates = []\n",
        "        for beam_idx, (sequence, log_prob) in enumerate(beams):\n",
        "            if sequence.size(1) > model.config.block_size:\n",
        "                idx_cond = sequence[:, -model.config.block_size:]\n",
        "            else:\n",
        "                idx_cond = sequence\n",
        "            logits, _ = model(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            # Apply repetition penalty\n",
        "            if repetition_penalty != 1.0:\n",
        "              # Get unique tokens in the generated sequence\n",
        "              unique_tokens = torch.unique(x[0])\n",
        "\n",
        "              # Create a mask for the tokens that have been used\n",
        "              mask = torch.zeros_like(logits[0]).bool()\n",
        "              mask[unique_tokens] = True\n",
        "\n",
        "              # Apply the penalty only to the used tokens\n",
        "              logits[0, mask] /= repetition_penalty\n",
        "\n",
        "            probs = F.log_softmax(logits, dim=-1)\n",
        "            top_probs, top_indices = probs.topk(beam_width)\n",
        "            for prob, idx in zip(top_probs[0], top_indices[0]):\n",
        "                new_sequence = torch.cat((sequence, idx.unsqueeze(0).unsqueeze(0)), dim=1)\n",
        "                new_log_prob = log_prob + prob.item()\n",
        "                candidates.append((new_sequence, new_log_prob))\n",
        "        beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "    return beams[0][0]\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, x, max_new_tokens, temperature=1.0, top_k=None, top_p=None, beam_width=None, repetition_penalty=1.0, greedy=False):\n",
        "    print(f\"Starting generation with max_new_tokens={max_new_tokens}\")\n",
        "\n",
        "    if beam_width != 0:\n",
        "        print(f\"Using beam search with width {beam_width}\")\n",
        "        return beam_search(model, x, max_new_tokens, beam_width, repetition_penalty)\n",
        "\n",
        "    for i in range(max_new_tokens):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Generated {i} tokens\")\n",
        "\n",
        "        if x.size(1) > model.config.block_size:\n",
        "            idx_cond = x[:, -model.config.block_size:]\n",
        "        else:\n",
        "            idx_cond = x\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        # Apply repetition penalty\n",
        "        if repetition_penalty != 1.0:\n",
        "          # Get unique tokens in the generated sequence\n",
        "          unique_tokens = torch.unique(x[0])\n",
        "\n",
        "          # Create a mask for the tokens that have been used\n",
        "          mask = torch.zeros_like(logits[0]).bool()\n",
        "          mask[unique_tokens] = True\n",
        "\n",
        "          # Apply the penalty only to the used tokens\n",
        "          logits[0, mask] /= repetition_penalty\n",
        "\n",
        "        if greedy:\n",
        "            # Greedy decoding: select the token with the highest probability\n",
        "            idx_next = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
        "        else:\n",
        "            # Existing sampling methods\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            if top_p != 0.0:\n",
        "                idx_next = top_p_sampling(logits, top_p)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        x = torch.cat((x, idx_next), dim=1)\n",
        "\n",
        "    print(\"Generation complete\")\n",
        "    return x\n",
        "\n",
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            print(f\"Generating sample {k+1}/{num_samples}\")\n",
        "            y = generate(model, x, max_new_tokens, temperature=temperature, top_k=top_k, top_p=top_p,\n",
        "                         beam_width=beam_width, repetition_penalty=repetition_penalty, greedy=greedy)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')\n",
        "'''\n",
        "\n",
        "with open('extended_sample.py', 'w') as f:\n",
        "  f.write(new_sample_script)"
      ],
      "metadata": {
        "id": "k4N5X2KDQ-4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanilla Sample Generation\n",
        "\n",
        "```python\n",
        "@torch.no_grad()\n",
        "def generate(model, x, max_new_tokens, temperature=1.0, top_k=None, top_p=None, beam_width=None, repetition_penalty=1.0, greedy=False):\n",
        "    print(f\"Starting generation with max_new_tokens={max_new_tokens}\")\n",
        "    \n",
        "    if beam_width != 0:\n",
        "        print(f\"Using beam search with width {beam_width}\")\n",
        "        return beam_search(model, x, max_new_tokens, beam_width, repetition_penalty)\n",
        "\n",
        "    for i in range(max_new_tokens):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Generated {i} tokens\")\n",
        "        \n",
        "        if x.size(1) > model.config.block_size:\n",
        "            idx_cond = x[:, -model.config.block_size:]\n",
        "        else:\n",
        "            idx_cond = x\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        # Apply repetition penalty\n",
        "        if repetition_penalty != 1.0:\n",
        "          # Get unique tokens in the generated sequence\n",
        "          unique_tokens = torch.unique(x[0])\n",
        "          \n",
        "          # Create a mask for the tokens that have been used\n",
        "          mask = torch.zeros_like(logits[0]).bool()\n",
        "          mask[unique_tokens] = True\n",
        "          \n",
        "          # Apply the penalty only to the used tokens\n",
        "          logits[0, mask] /= repetition_penalty\n",
        "\n",
        "        if greedy:\n",
        "            # Greedy decoding: select the token with the highest probability\n",
        "            idx_next = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
        "        else:\n",
        "            # Existing sampling methods\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            if top_p != 0.0:\n",
        "                idx_next = top_p_sampling(logits, top_p)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        x = torch.cat((x, idx_next), dim=1)\n",
        "\n",
        "    print(\"Generation complete\")\n",
        "    return x\n",
        "```\n",
        "\n",
        "As we can see - if we *don't* use a `top_p`, `top_k`, or `beam_width` - we simply sample the distribution as-in."
      ],
      "metadata": {
        "id": "KrdbM6PLf3B3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python extended_sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl_maPrbRbjJ",
        "outputId": "b11ceb5b-9179-4bbb-9922-bd063c9015cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "2024-12-07 21:35:17.065539: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-07 21:35:17.082470: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 21:35:17.103361: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 21:35:17.109737: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 21:35:17.124709: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 21:35:18.312617: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Generating sample 1/1\n",
            "Starting generation with max_new_tokens=100\n",
            "Generated 0 tokens\n",
            "Generation complete\n",
            "What is the answer to life, the universe, and everything?\n",
            "\n",
            "One possibility is that they have a universe-wide perspective that allows for no contradictions. And, if so, then why is there contradiction?\n",
            "\n",
            "Another possibility is that the universe is one big joke. They are not the only joke in the universe. But it is not an empty universe.\n",
            "\n",
            "Hence the problems with the universe's Big Bang theory, which tells us that the universe started out in a singularity with a zero initial mass. The Big Bang is essentially consistent with\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greedy Decoding\n",
        "\n",
        "Greedy decoding is the simplest method for generating text from a language model. It works as follows:\n",
        "\n",
        "1. **Prediction**: At each step, the model predicts the probability distribution for the next token.\n",
        "\n",
        "2. **Selection**: The token with the highest probability is selected.\n",
        "\n",
        "3. **Iteration**: This process is repeated until the desired length is reached or a stop condition is met.\n",
        "\n",
        "Key characteristics of greedy decoding:\n",
        "\n",
        "- **Deterministic**: Given the same input and model state, it always produces the same output.\n",
        "- **Fast**: It's computationally efficient as it doesn't require sampling or complex calculations.\n",
        "- **Lack of diversity**: It tends to generate repetitive and sometimes boring text, especially for longer sequences.\n",
        "\n",
        "Let's check out the code!\n",
        "\n",
        "```python\n",
        "if greedy:\n",
        "    # Greedy decoding: select the token with the highest probability\n",
        "    idx_next = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
        "```"
      ],
      "metadata": {
        "id": "aR0f32wMmJ3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And see it in practice!"
      ],
      "metadata": {
        "id": "We72GXX1n1Wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python extended_sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100 --greedy=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI8jlgeCSD8Q",
        "outputId": "1182e709-8408-4ad4-e29e-6898c65f434b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "Overriding: greedy = True\n",
            "2024-12-07 21:36:03.937973: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-07 21:36:03.954784: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 21:36:03.975594: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 21:36:03.981895: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 21:36:03.996701: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 21:36:05.175729: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Generating sample 1/1\n",
            "Starting generation with max_new_tokens=100\n",
            "Generated 0 tokens\n",
            "Generation complete\n",
            "What is the answer to life, the universe, and everything?\n",
            "\n",
            "The answer is that we don't know.\n",
            "\n",
            "We don't know what the universe is made of, or how it came to be.\n",
            "\n",
            "We don't know what the laws of nature are.\n",
            "\n",
            "We don't know what the purpose of life is.\n",
            "\n",
            "We don't know what the meaning of life is.\n",
            "\n",
            "We don't know what the purpose of the universe is.\n",
            "\n",
            "We don't know what the purpose of the universe is.\n",
            "\n",
            "\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is a bit repetitive, is there way we can reduce this?"
      ],
      "metadata": {
        "id": "a4SjjQNtlOyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repetition Penalty\n",
        "\n",
        "Repetition penalty is a technique used in text generation to reduce the likelihood of a model repeating the same words or phrases too frequently. It works by dynamically adjusting the probability of tokens that have already been generated.\n",
        "\n",
        "Key aspects of repetition penalty:\n",
        "\n",
        "1. **Penalization**: It decreases the probability of tokens that have appeared in the generated text.\n",
        "2. **Adjustable strength**: The penalty can be tuned to control the trade-off between coherence and diversity.\n",
        "3. **Applicability**: It can be applied alongside other decoding methods like greedy search, beam search, or sampling techniques.\n",
        "\n",
        "The basic idea is to divide the logits (unnormalized prediction scores) of previously generated tokens by a penalty factor.\n",
        "\n",
        "```python\n",
        "if repetition_penalty != 1.0:\n",
        "    # Consider only the last 1000 tokens (adjust as needed)\n",
        "    window_size = 1000\n",
        "    recent_tokens = x[0, -window_size:]\n",
        "    \n",
        "    # Get unique tokens in the recent sequence\n",
        "    unique_tokens = torch.unique(recent_tokens)\n",
        "    \n",
        "    # Create a mask for the tokens that have been used recently\n",
        "    mask = torch.zeros_like(logits[0]).bool()\n",
        "    mask[unique_tokens] = True\n",
        "    \n",
        "    # Apply the penalty only to the recently used tokens\n",
        "    logits[0, mask] /= repetition_penalty\n",
        "```"
      ],
      "metadata": {
        "id": "3waoOo9Hn5F7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see it in practice!"
      ],
      "metadata": {
        "id": "rsRe-kI4oC8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python extended_sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100 --greedy=True --repetition_penalty=1.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22Xdcf8JlqCp",
        "outputId": "d9dbbe74-2103-4fe6-9564-02840c535257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "Overriding: greedy = True\n",
            "Overriding: repetition_penalty = 1.8\n",
            "2024-12-07 21:36:50.714875: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-07 21:36:50.731048: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 21:36:50.751465: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 21:36:50.757667: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 21:36:50.772129: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 21:36:51.974846: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Generating sample 1/1\n",
            "Starting generation with max_new_tokens=100\n",
            "Generated 0 tokens\n",
            "Generation complete\n",
            "What is the answer to life, the universe, and everything?\n",
            "\n",
            "The question of what we are made from has been a central one for humanity since our earliest days. The first humans were thought by many people at that time (and still today) as being \"primitive\" or having no souls; they had only bodies with which their minds could communicate through touch alone.[1] However this view was quickly overturned when modern science began its study on human anatomy in earnest during the 19th century,[2][3]. It became clear over these centuries how much\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-P (Nucleus) Sampling\n",
        "\n",
        "Top-P sampling, also known as nucleus sampling, is a text generation technique that provides a balance between diversity and quality in the generated text. It works as follows:\n",
        "\n",
        "1. **Sorting probabilities**: After the model produces the probability distribution for the next token, the probabilities are sorted in descending order.\n",
        "\n",
        "2. **Cumulative sum**: A cumulative sum of these sorted probabilities is calculated.\n",
        "\n",
        "3. **Probability mass selection**: A threshold p (typically between 0.9 and 1) is chosen. The smallest set of tokens whose cumulative probability exceeds p is selected.\n",
        "\n",
        "4. **Sampling**: The next token is randomly sampled from this reduced set of tokens.\n",
        "\n",
        "Key advantages of Top-P sampling:\n",
        "\n",
        "- It adapts to the confidence of the model's predictions.\n",
        "- It can produce more diverse outputs than methods like Top-K sampling, especially for less confident predictions.\n",
        "- It helps avoid low-probability tokens while maintaining a dynamic vocabulary size.\n",
        "\n",
        "```python\n",
        "def top_p_sampling(logits, p):\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "    sorted_indices_to_remove = cumulative_probs > p\n",
        "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "    sorted_indices_to_remove[..., 0] = 0\n",
        "    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "    probs = probs.masked_fill(indices_to_remove, 0.0)\n",
        "    return torch.multinomial(probs, num_samples=1)\n",
        "```"
      ],
      "metadata": {
        "id": "nsOQSY6jm_yY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see it in action!"
      ],
      "metadata": {
        "id": "sCMuIwdXoXoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python extended_sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100 --top_p=0.95"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8sfAMjToaCo",
        "outputId": "dd63c0c8-61ba-4943-ea65-f6fb216257b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "Overriding: top_p = 0.95\n",
            "2024-12-07 21:37:38.146622: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-07 21:37:38.163507: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 21:37:38.184359: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 21:37:38.190663: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 21:37:38.205608: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 21:37:39.400867: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Generating sample 1/1\n",
            "Starting generation with max_new_tokens=100\n",
            "Generated 0 tokens\n",
            "Generation complete\n",
            "What is the answer to life, the universe, and everything?\n",
            "\n",
            "One possibility is that they have a universe-wide perspective that allows them to think in general.\n",
            "\n",
            "If that's the case, then an individual's life might seem like nothing but one tiny drop in a vast ocean of universes.\n",
            "\n",
            "But if that's not the case, then the answer is still far from clear.\n",
            "\n",
            "So far, the universe is the only place we've ever known, and it has caused us no problem so far.\n",
            "\n",
            "So does that mean\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see this with a low temperature, and a high temperature and observe the difference!"
      ],
      "metadata": {
        "id": "1disLRV_o-AQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cold as ice:"
      ],
      "metadata": {
        "id": "-xKzCGWspG2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python extended_sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100 --top_p=0.95 --temperature=0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqmAjRNUpCJ5",
        "outputId": "8a05de24-4e55-4936-a58a-5cd9ae28a963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "Overriding: top_p = 0.95\n",
            "Overriding: temperature = 0.1\n",
            "2024-12-07 21:38:25.299689: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-07 21:38:25.316511: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 21:38:25.337622: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 21:38:25.343956: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 21:38:25.359052: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 21:38:26.543351: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Generating sample 1/1\n",
            "Starting generation with max_new_tokens=100\n",
            "Generated 0 tokens\n",
            "Generation complete\n",
            "What is the answer to life, the universe, and everything?\n",
            "\n",
            "The answer is that we don't know.\n",
            "\n",
            "We don't know what the universe is made of, or how it came to be.\n",
            "\n",
            "We don't know what the laws of nature are.\n",
            "\n",
            "We don't know what the laws of physics are.\n",
            "\n",
            "We don't know what the laws of chemistry are.\n",
            "\n",
            "We don't know what the laws of biology are.\n",
            "\n",
            "We don't know what the laws of physics are.\n",
            "\n",
            "We don\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Burning hot:"
      ],
      "metadata": {
        "id": "3ZhcKjVgpIVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python extended_sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100 --top_p=0.05 --temperature=100.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "884hReuEpFmG",
        "outputId": "e223cdf5-56bd-4d50-e06d-867eeafb54a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "Overriding: top_p = 0.05\n",
            "Overriding: temperature = 100.0\n",
            "2024-12-07 21:39:12.285171: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-07 21:39:12.302574: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 21:39:12.324065: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 21:39:12.330548: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 21:39:12.345836: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 21:39:13.536279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Generating sample 1/1\n",
            "Starting generation with max_new_tokens=100\n",
            "Generated 0 tokens\n",
            "Generation complete\n",
            "What is the answer to life, the universe, and everything? It may come, according to a newly developed theory which could solve all mysteries that physicists are seeking to resolve about why and by why we are here.\"The Big Question, the first ever paper by Nobel prize physicist Steven Jaki on 'Life as You Have Never Kneeled before: An Introduction into the Physics that Drices the Creation, and the Endeavor to Know About the Origin (PDF)\" was submitted by Jakis as he submitted the article and is being posted here on our main page\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search\n",
        "\n",
        "Beam search is a heuristic search algorithm commonly used in natural language processing for text generation tasks. It's an extension of greedy decoding that explores multiple possible sequences simultaneously.\n",
        "\n",
        "Key concepts:\n",
        "\n",
        "1. **Beam width**: The number of sequences to keep track of at each step.\n",
        "2. **Expanding**: At each step, generate all possible next tokens for each sequence in the beam.\n",
        "3. **Pruning**: Keep only the top-k sequences (where k is the beam width) based on their cumulative probability scores.\n",
        "\n",
        "Algorithm overview:\n",
        "\n",
        "1. Start with an initial sequence (usually a start token).\n",
        "2. Generate the next token probabilities for each sequence in the beam.\n",
        "3. Create new candidate sequences by appending each possible next token.\n",
        "4. Score each candidate sequence based on its cumulative log probability.\n",
        "5. Select the top-k sequences to form the new beam.\n",
        "6. Repeat steps 2-5 until the desired length is reached or a stop condition is met."
      ],
      "metadata": {
        "id": "lTp4D2iBoZIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python extended_sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=1 --max_new_tokens=50 --beam_width=16 --repetition_penalty=1.8 --temperature=1.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L06prEqhToUj",
        "outputId": "ee4acac3-9ce2-442d-bca0-6bdd66cb4cab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 50\n",
            "Overriding: beam_width = 16\n",
            "Overriding: repetition_penalty = 1.8\n",
            "Overriding: temperature = 1.4\n",
            "2024-12-07 21:40:37.153087: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-07 21:40:37.170000: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 21:40:37.190940: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 21:40:37.197265: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 21:40:37.212247: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 21:40:38.385029: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Generating sample 1/1\n",
            "Starting generation with max_new_tokens=50\n",
            "Using beam search with width 16\n",
            "What is the answer to life, the universe, and everything?\n",
            "\n",
            "I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python extended_sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"Why is the sky blue?\" \\\n",
        "    --num_samples=1 --max_new_tokens=50 --beam_width=16 --repetition_penalty=1.8 --temperature=1.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_-bvuRLvURk",
        "outputId": "48eeb3fc-f1e1-42f9-e2d2-f4cf6d9ca15d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = Why is the sky blue?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 50\n",
            "Overriding: beam_width = 16\n",
            "Overriding: repetition_penalty = 1.8\n",
            "Overriding: temperature = 1.4\n",
            "2024-12-07 21:41:41.443698: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-07 21:41:41.460560: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-07 21:41:41.481288: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-07 21:41:41.487563: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-07 21:41:41.502386: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-07 21:41:42.664827: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Generating sample 1/1\n",
            "Starting generation with max_new_tokens=50\n",
            "Using beam search with width 16\n",
            "Why is the sky blue?\n",
            "\n",
            "The answer to this question depends on what you mean by \"sky\" and what you mean by \"blue\".\n",
            "\n",
            "The word \"sky\" has two different meanings in English.\n",
            "\n",
            "The first meaning of \"sky\" refers to a\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ğŸ‘ªâ“ Discussion Question #2:\n",
        "\n",
        "Which decoding method is *best*? If there is no obvious best, please list the pros/cons/use-cases for each method.\n",
        "\n",
        "There is no obvious \"best\" method when it comes to sampling and it depends on the nature of the application we're building and our compute budget. Examining some pros and cons of each approach:\n",
        "\n",
        "1. Greedy - This sampling method should typically only be used for prediction of very short sequences of tokens (ideally 1-3) because of how much it optimizes for the next token. It optimizes for the short horizon and can lead to issues with longer sequences. However, it is computationally very efficient.\n",
        "2. Sampling from the distribution (no changes) - This method can be a good balance between greedy and more computationally complex methods which may be computationally expensive. If we believe that the learned distribution of the model is decent, then this method might be effective. However, it may lead to some meaningless or repetitive outputs since it has no repetition penalty.\n",
        "3. top-k and top-p - Both of these methods can be a good balance between greedy and sampling. Their use can lead to fewer surprises as they still sample most likely outcomes. However, their use with a low temperature might lead to repetituion. This can be countered with repetition penalty.\n",
        "4. Beam Search - This method is most interesting for predicting long sequences because it optimizes for the joint probabilty of the sequence as opposed to the next token. It may produce some surprising results but they can be countered if it is used together with other parameters like repetion penalty, top-p and topk. The downside is that it is extremely computationally expensive and can still produce some repetitive results.\n",
        "5. Temperature and Repetition Penalty - These are less sampling methods but rather parameters which can be used with other methods. Use of high(er) temperature (>1) can encourage the model to be more creative. Repetition penalty can avoid repeated outputs and counter the \"greedy\" effect in sampling.\n",
        "\n",
        "If I had to pick one method only (although I don't know why I would restrict myself in this way), I would probably use top-p with a temperature > 1 and some repetition penalty as a default. However, some tuning would be required depending on the model to find the best defaults."
      ],
      "metadata": {
        "id": "OCylr1WluXw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Breakout Room #2:"
      ],
      "metadata": {
        "id": "FQIbF5cr9CA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Speculative Decoding\n",
        "\n",
        "Speculative decoding is a technique that aims to speed up text generation by using a smaller, faster \"draft\" model to predict multiple tokens at once, which are then verified by a larger, more accurate model. Here's how it works:\n",
        "\n",
        "1. Draft Stage:\n",
        "\n",
        "- A smaller, faster model proposes a sequence of tokens\n",
        "- The draft model can generate these quickly but with lower quality\n",
        "- The number of tokens generated is a tunable parameter (num_draft_tokens)\n",
        "\n",
        "\n",
        "2. Verification Stage:\n",
        "\n",
        "- A larger, more accurate model evaluates the proposed tokens\n",
        "- For each token, it calculates the probability ratio between the draft and verifier models\n",
        "- Tokens are accepted if the ratio meets certain criteria\n",
        "\n",
        "\n",
        "3. Accept/Reject Process:\n",
        "\n",
        "- Accepted tokens are added to the sequence\n",
        "- If a token is rejected, we fall back to the verifier's prediction\n",
        "- The process then continues from the last accepted token\n",
        "\n",
        "\n",
        "**Key advantages**:\n",
        "\n",
        "- Can provide significant speedup (2-3x) over traditional decoding\n",
        "- Maintains quality of the larger model\n",
        "- Particularly effective for long sequences"
      ],
      "metadata": {
        "id": "t_N6JOYIq2oz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### âš ï¸ Hardmode Challenge:\n",
        "\n",
        "Please implement speculative decoding - you can assume that `gpt-2` and `gpt2-medium` are compatible enough for this process to loosely work."
      ],
      "metadata": {
        "id": "dr6yNcRo_B0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import gc\n",
        "\n",
        "class DirectSpeculativeDecoder:\n",
        "    def __init__(self, draft_model_name='gpt2', verifier_model_name='gpt2-medium', num_draft_tokens=4,\n",
        "                 loss_model_name='gpt2-medium'):\n",
        "        \"\"\"Initialize decoder with draft and verifier models\"\"\"\n",
        "        # Clear CUDA cache and garbage collect\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Set model precision\n",
        "        self.dtype = torch.float16 if self.device == 'cuda' else torch.float32\n",
        "\n",
        "        # Load models and tokenizer with proper memory handling\n",
        "        print(f\"Loading draft model: {draft_model_name}\")\n",
        "        self.draft_model = GPT2LMHeadModel.from_pretrained(\n",
        "            draft_model_name,\n",
        "            torch_dtype=self.dtype,\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "        self.draft_model.eval().to(self.device)\n",
        "\n",
        "        print(f\"Loading verifier model: {verifier_model_name}\")\n",
        "        self.verifier_model = GPT2LMHeadModel.from_pretrained(\n",
        "            verifier_model_name,\n",
        "            torch_dtype=self.dtype,\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "        self.verifier_model.eval().to(self.device)\n",
        "\n",
        "        print(f\"Loading loss model: {loss_model_name}\")\n",
        "        self.loss_model = GPT2LMHeadModel.from_pretrained(\n",
        "            loss_model_name,\n",
        "            torch_dtype=self.dtype,\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "        self.loss_model.eval().to(self.device)\n",
        "\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(draft_model_name)\n",
        "        self.num_draft_tokens = num_draft_tokens\n",
        "\n",
        "        # Add padding token if it doesn't exist\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def _safe_model_output(self, model, input_ids):\n",
        "        \"\"\"Safely get model outputs with proper error handling\"\"\"\n",
        "        try:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(input_ids)\n",
        "            return outputs\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Error during model inference: {str(e)}\")\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            raise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, prompt, max_new_tokens, temperature=1.0,\n",
        "                 use_verifier=True, return_loss=False):\n",
        "        \"\"\"Generate text using speculative decoding\"\"\"\n",
        "        try:\n",
        "            input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
        "            block_size = min(self.draft_model.config.n_ctx, self.verifier_model.config.n_ctx)\n",
        "            generated_tokens = 0\n",
        "            while generated_tokens < max_new_tokens:\n",
        "              draft_input_ids = input_ids.clone()\n",
        "              if draft_input_ids.size(1) >= block_size:\n",
        "                draft_input_ids = draft_input_ids[:, -block_size:]\n",
        "              initial_length = draft_input_ids.size(1)\n",
        "              draft_tokens = []\n",
        "              for _ in range(self.num_draft_tokens):\n",
        "                logits = self._safe_model_output(self.draft_model, draft_input_ids).logits[:, -1, :] / temperature\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                token = torch.multinomial(probs, num_samples=1)\n",
        "                draft_input_ids = torch.cat((draft_input_ids, token), dim=1)\n",
        "                draft_tokens.append(token.item())\n",
        "\n",
        "              if not use_verifier:\n",
        "                # For testing loss without using verifier\n",
        "                input_ids = torch.cat([input_ids, torch.tensor([draft_tokens], dtype=input_ids.dtype, device=input_ids.device)], dim=1)\n",
        "                generated_tokens += len(draft_tokens)\n",
        "                continue\n",
        "\n",
        "              # Note that the version of this algorithm in the regular notebook\n",
        "              # doesn't follow the psuedocode listed above. It continues to sample\n",
        "              # from the verification_probs even after a rejection. When generating\n",
        "              # longer samples, this is problematic. My version below, starts\n",
        "              # sampling again from the last accepted token in the event of a\n",
        "              # rejection. This leads to better outputs for longer sequences.\n",
        "\n",
        "              # Also note that the regular notebook makes use of config.max_length\n",
        "              # which in the case of gpt2 config isn't really the context length.\n",
        "\n",
        "              # Verification\n",
        "              verification_temperature = temperature/1.25 # Helps for verification model\n",
        "              verification_logits = self._safe_model_output(self.verifier_model, draft_input_ids).logits / verification_temperature\n",
        "              verification_probs = F.softmax(verification_logits, dim=-1)\n",
        "\n",
        "              # Accept/Reject\n",
        "\n",
        "              for i in range(self.num_draft_tokens):\n",
        "                token = draft_input_ids[0, initial_length + i].item()\n",
        "                if torch.rand(1).item() < verification_probs[0, initial_length + i - 1, token].item():\n",
        "                  # Accept the token\n",
        "                  input_ids = torch.cat([input_ids, torch.tensor([[token]], dtype=torch.long, device=self.device)], dim=1)\n",
        "                  generated_tokens += 1\n",
        "                else:\n",
        "                  new_token = torch.multinomial(verification_probs[:, initial_length + i - 1], 1)\n",
        "                  # Use newly generated token from verification model\n",
        "                  input_ids = torch.cat([input_ids, new_token], dim=1)\n",
        "                  generated_tokens += 1\n",
        "                  break\n",
        "\n",
        "            output_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "            if return_loss:\n",
        "              logits = self._safe_model_output(self.loss_model, input_ids).logits / temperature\n",
        "              logits = logits[:, :-1, :].contiguous()\n",
        "              input_ids = input_ids[:, 1:].contiguous()\n",
        "              input_ids = input_ids.view(-1)\n",
        "              logits = logits.view(-1, logits.size(-1))\n",
        "              loss = F.cross_entropy(logits, input_ids)\n",
        "              return output_text, loss.item()\n",
        "            else:\n",
        "              return output_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during generation: {str(e)}\")\n",
        "            if self.device == 'cuda':\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            raise\n"
      ],
      "metadata": {
        "id": "GC7kCovZ1zTe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we just need some code to actually run that beast!"
      ],
      "metadata": {
        "id": "-_IFUoig7Pdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_speculative_decoder(prompt, max_tokens=50, use_verifier=True, return_loss=True):\n",
        "    try:\n",
        "        decoder = DirectSpeculativeDecoder(\n",
        "            draft_model_name='gpt2',\n",
        "            verifier_model_name='gpt2-medium',\n",
        "            num_draft_tokens=4\n",
        "        )\n",
        "\n",
        "        print(\"\\nGenerating with prompt:\", prompt)\n",
        "        result = decoder.generate(\n",
        "            prompt=prompt,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=0.8,\n",
        "            use_verifier=use_verifier,\n",
        "            return_loss=return_loss,\n",
        "        )\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in speculative decoding: {str(e)}\")\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        raise"
      ],
      "metadata": {
        "id": "dEb6JWt66twu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can try it out!"
      ],
      "metadata": {
        "id": "rVA7n6om7SKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the answer to life, the universe, and everything?\"\n",
        "result, loss = run_speculative_decoder(prompt, max_tokens=100, return_loss=True)\n",
        "print(\"\\nLoss:\", loss)\n",
        "print(\"\\nFinal result: \", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBQh62Vp2Y8L",
        "outputId": "6bd1abfa-cc73-4df0-9097-b991029a73ce"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading draft model: gpt2\n",
            "Loading verifier model: gpt2-medium\n",
            "Loading loss model: gpt2-medium\n",
            "\n",
            "Generating with prompt: What is the answer to life, the universe, and everything?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-23bfba24e74d>:56: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loss: 1.2001953125\n",
            "\n",
            "Final result:  What is the answer to life, the universe, and everything?\n",
            "\n",
            "In this episode, we begin our journey into the universe and its mysteries. We learn about our universe, how it got here, and where we are going. We learn about the universe we live in, what the universe is, and why we are here. We learn about our existence, and why we exist. We learn about what it means to be alive, and what it means to be dead. We learn about the things we do, and how they interact with each other. We learn about the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a test, let's also generate a completion using the draft model only without using the verifier and compare outputs and loss."
      ],
      "metadata": {
        "id": "RIFbi99P7Ir-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the answer to life, the universe, and everything?\"\n",
        "result, draft_loss = run_speculative_decoder(prompt, max_tokens=100, return_loss=True, use_verifier=False)\n",
        "print(\"\\nLoss:\", draft_loss)\n",
        "print(\"\\nFinal result: \", result)"
      ],
      "metadata": {
        "id": "TTPtCZ0s5ITx",
        "outputId": "326949a8-a04a-401a-bdca-385db8c66ada",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading draft model: gpt2\n",
            "Loading verifier model: gpt2-medium\n",
            "Loading loss model: gpt2-medium\n",
            "\n",
            "Generating with prompt: What is the answer to life, the universe, and everything?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-23bfba24e74d>:56: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final result: ('What is the answer to life, the universe, and everything?\\n\\nIt all depends on what you\\'re trying to say, but it\\'s everybody\\'s choice. If you have kids now, they will have kids at some point. It would be easy to say, \"Okay, let\\'s run a $3,500 bet on this,\" but if you\\'re saying, \"Yes, I\\'ll run a $3,500 bet on this, and if I did it, there\\'d be a $3,500 bonus,\" you\\'re only saying, \"Well', 2.146484375)\n",
            "\n",
            "Loss: 2.146484375\n",
            "\n",
            "Final result:  What is the answer to life, the universe, and everything?\n",
            "\n",
            "It all depends on what you're trying to say, but it's everybody's choice. If you have kids now, they will have kids at some point. It would be easy to say, \"Okay, let's run a $3,500 bet on this,\" but if you're saying, \"Yes, I'll run a $3,500 bet on this, and if I did it, there'd be a $3,500 bonus,\" you're only saying, \"Well\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the loss without using verifier is higher. However, this methodology is a bit flawed because we expect the loss to be lower when we're using gpt2-medium as the loss predictor above. It does tell us though that there isn't some underlying bug in the generation.\n",
        "\n",
        "A more appropriate way to compute loss might be to use an independent model (like Eleuther's Pythia family) and compute loss or perplexity on the generated text."
      ],
      "metadata": {
        "id": "5wzJzr6M7TI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### â“ Question #2:\n",
        "\n",
        "How does this method offer a speed-up over traditional decoding?\n",
        "\n",
        "The key idea is that a smaller, much faster language model can be used for generating max_new_tokens tokens. For each of these tokens, we're doing an autoregressive inference, but the model is faster than a larger LM.\n",
        "\n",
        "Thereafter, a larger model can be used to compute the logits of the draft sequence which the smaller model produced. Since draft sequence is already generated, we only need one inference using the larger model to produce logits, and thus softmax probabilities of each token in the sequence. Then, we can threshold the these probabilities and decide whether we want to keep the draft token or sample from the distribution produced by the verifier model.\n",
        "\n",
        "Note that we only needed to do a single inference over the larger model. Thus most of the decoding was done by the smaller model. This is what makes this algorithm efficient."
      ],
      "metadata": {
        "id": "-U5tkFXxr_2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 6: Guard Rails\n",
        "\n",
        "Guard Rails is a technique for controlling the output of language models by filtering or constraining the tokens that can be generated. This provides a way to ensure outputs meet certain criteria without having to retrain or fine-tune the model.\n",
        "\n",
        "\n",
        "Key components:\n",
        "\n",
        "1. Token-Level Filtering:\n",
        "\n",
        "- Maintains a set of blocked token IDs\n",
        "- Applies filtering during the generation process\n",
        "- Can handle both individual tokens and sequences\n",
        "\n",
        "\n",
        "2. Vocabulary Management:\n",
        "\n",
        "- Converts blocked words to token IDs\n",
        "- Handles different forms of words (prefixes, suffixes)\n",
        "- Works with the model's tokenizer\n",
        "\n",
        "\n",
        "3. Probability Modification:\n",
        "\n",
        "- Sets probabilities of blocked tokens to -inf\n",
        "- Preserves relative probabilities of allowed tokens\n",
        "- Integrates with temperature and top-k sampling"
      ],
      "metadata": {
        "id": "WJU15zam1ajP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### âš ï¸ Hardmode Challenge:\n",
        "\n",
        "Please implement the code required for `TokenGuardRails`."
      ],
      "metadata": {
        "id": "NuFLOlcP_eQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TokenGuardRails:\n",
        "    def __init__(self, tokenizer, blocked_token_ids=None):\n",
        "        \"\"\"\n",
        "        Initialize guard rails with blocked token IDs\n",
        "\n",
        "        Args:\n",
        "            tokenizer: The tokenizer to use for converting between tokens and text\n",
        "            blocked_token_ids (set): Set of token IDs that should be blocked\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.blocked_token_ids = set(blocked_token_ids or [])\n",
        "\n",
        "    def filter_logits(self, logits):\n",
        "        \"\"\"\n",
        "        Filter logits by setting probabilities of blocked tokens to -inf\n",
        "\n",
        "        Args:\n",
        "            logits (torch.Tensor): Raw logits from model [batch_size, vocab_size]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Filtered logits with same shape\n",
        "        \"\"\"\n",
        "        mask = torch.zeros_like(logits[0]).bool()\n",
        "        mask[list(self.blocked_token_ids)] = True\n",
        "        logits[0, mask] = -float('inf')\n",
        "        return logits\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_with_token_guard_rails(model, x, guard_rails, max_new_tokens, temperature=1.0, top_k=None):\n",
        "    \"\"\"\n",
        "    Generate text using token-level guard rails\n",
        "\n",
        "    Args:\n",
        "        model: The language model\n",
        "        x (torch.Tensor): Input token IDs [batch_size, seq_len]\n",
        "        guard_rails (TokenGuardRails): Guard rails instance\n",
        "        max_new_tokens (int): Maximum number of tokens to generate\n",
        "        temperature (float): Sampling temperature\n",
        "        top_k (int): If set, use top-k filtering\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      with torch.autocast(\"cuda\"):\n",
        "        if x.size(1) >= model.config.n_ctx:\n",
        "          x = x[:, -model.config.n_ctx:]\n",
        "        logits = model(x).logits[:, -1, :] / temperature\n",
        "        logits = guard_rails.filter_logits(logits)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        if top_k is not None:\n",
        "          v, _ = torch.topk(probs, k=top_k, dim=-1)\n",
        "          logits[logits < v[:, -1]] = float('-inf')\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        x = torch.cat((x, next_token), dim=1)\n",
        "    return x\n",
        "\n",
        "def setup_token_guard_rails(tokenizer, blocked_words):\n",
        "    \"\"\"Helper to set up token guard rails from word list\"\"\"\n",
        "    blocked_token_ids = set()\n",
        "    for word in blocked_words:\n",
        "      token = tokenizer.encode(word, add_special_tokens=False)[0]\n",
        "      blocked_token_ids.add(token)\n",
        "    return blocked_token_ids\n",
        "\n",
        "\n",
        "def run_guarded_generation(prompt, model, tokenizer, blocked_words, max_new_tokens=100):\n",
        "    \"\"\"Run generation with token guard rails\"\"\"\n",
        "    blocked_token_ids = setup_token_guard_rails(tokenizer, blocked_words)\n",
        "    guard_rails = TokenGuardRails(tokenizer, blocked_token_ids)\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
        "    result = generate_with_token_guard_rails(model, input_ids, guard_rails, max_new_tokens)\n",
        "    return tokenizer.decode(result[0])"
      ],
      "metadata": {
        "id": "cfmM5TxL5Cov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key advantages:**\n",
        "\n",
        "- Fine-grained control over model outputs\n",
        "- No need for model retraining\n",
        "- Can be updated dynamically\n",
        "- Maintains coherence of generated text"
      ],
      "metadata": {
        "id": "UDA0j74x7jYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Initialize model and tokenizer\n",
        "model_name = \"gpt2-medium\"  # Using smaller model for demonstration\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Test generation with guard rails\n",
        "blocked_words = [\"war\", \"conflict\", \"fight\", \"battle\"]\n",
        "prompt = \"The future of international relations will be characterized by\"\n",
        "\n",
        "print(\"Generating with blocked words:\", blocked_words)\n",
        "print(\"\\nPrompt:\", prompt)\n",
        "print(\"\\nGenerating...\")\n",
        "\n",
        "result = run_guarded_generation(\n",
        "    prompt=prompt,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    blocked_words=blocked_words,\n",
        "    max_new_tokens=50\n",
        ")\n",
        "\n",
        "print(\"\\nGenerated text:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAs9C2TS5Fju",
        "outputId": "ae7a319b-fac9-4885-ecda-8a405bea36b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating with blocked words: ['war', 'conflict', 'fight', 'battle']\n",
            "\n",
            "Prompt: The future of international relations will be characterized by\n",
            "\n",
            "Generating...\n",
            "\n",
            "Generated text: The future of international relations will be characterized by, as all market economies do, value-sequencing. A stable price, an ever-increasing quantity of money, officially fixed at the international level and from there depreciated (together with other factors of production), is necessary for the whole market\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ğŸ‘ªâ“ Question #4:\n",
        "\n",
        "What other applications could Guard Rails have (specifically output Guard Rails)?\n",
        "\n",
        "1. Guard Rails can make a model \"safer\" by filtering out subject matters or NSFW content by filtering out restricted keywords.\n",
        "2. Aside from filtering, Guard Rails can also be used to modify (increase or decrease) the probability of tokens at specific positions, thus it can be used to make the model output adhere to a specific structure."
      ],
      "metadata": {
        "id": "NwfYpiibxVeB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BvYN9bE3FBcz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}